Script started on Wed 24 Jan 2018 09:58:58 AM CST
]0;sid@blueberry ~/tensorflow2.7/projects/tutorials/Tensorflow-tutorials/tutorial-2-image-classifier/training_data/dogs[01;32msid@blueberry[00m [01;34m~/tensorflow2.7/projects/tutorials/Tensorflow-tutorials/tutorial-2-image-classsifier/training_data/dogs $[00m vim tra.[Ki[K[K[K[K[K[K[K[Kcd ../../
]0;sid@blueberry ~/tensorflow2.7/projects/tutorials/Tensorflow-tutorials/tutorial-2-image-classifier[01;32msid@blueberry[00m [01;34m~/tensorflow2.7/projects/tutorials/Tensorflow-tutorials/tutorial-2-image-classsifier $[00m ls
[0m[01;35mcat.jpg[0m      [01;34mtesting_data[0m
checkpoint   test_model.ckpt.data-00000-of-00001
dataset.py   test_model.ckpt.data-00000-of-00001.tempstate1180099317889776508
dataset.pyc  test_model.ckpt.index
[01;35mdog2.jpg[0m     test_model.ckpt.meta
[01;35mdog.jpg[0m      [01;34mtraining_data[0m
predict.py   train.py
]0;sid@blueberry ~/tensorflow2.7/projects/tutorials/Tensorflow-tutorials/tutorial-2-image-classifier[01;32msid@blueberry[00m [01;34m~/tensorflow2.7/projects/tutorials/Tensorflow-tutorials/tutorial-2-image-classsifier $[00m vim train.py 
[?1049h[?1h=[2;1Hâ–½[6n[2;1H  [1;1H]11;?[1;46r[?12;25h[?12l[?25h[27m[23m[m[H[2J[?25l[46;1H"train.py" 205L, 6694C[>c[27m[23m[m[H[2J[1;1H[38;5;81mimport[m dataset
[38;5;81mimport[m tensorflow [93mas[m tf
[38;5;81mimport[m time
[38;5;81mfrom[m datetime [38;5;81mimport[m timedelta
[38;5;81mimport[m math
[38;5;81mimport[m random
[38;5;81mimport[m numpy [93mas[m np

[96m#Adding Seed so that random initialization is consistent[m
[38;5;81mfrom[m numpy.random [38;5;81mimport[m seed
seed([95m1[m)
[38;5;81mfrom[m tensorflow [38;5;81mimport[m set_random_seed
set_random_seed([95m2[m)


batch_size = [95m32[m

[96m#Prepare input data[m
classes = [[95m'dogs'[m,[95m'cats'[m]
num_classes = [1m[96mlen[m(classes)

[96m# 20% of the data will automatically be used for validation[m
validation_size = [95m0.2[m
img_size = [95m128[m
num_channels = [95m3[m
train_path=[95m"training_data"[m

[96m# We shall load all the training and validation images and labels into memory using openCVV[29;1H and use that during training[m
data = dataset.read_train_sets(train_path, img_size, classes, validation_size=validation_ss[31;1Hize)


[1m[96mprint[m([95m"Complete reading input data. Will Now print a snippet of it"[m)
[1m[96mprint[m([95m"Number of files in Training-set:[m[38;5;224m\t\t[m[95m{}"[m.format([1m[96mlen[m(data.train.labels)))
[1m[96mprint[m([95m"Number of files in Validation-set:[m[38;5;224m\t[m[95m{}"[m.format([1m[96mlen[m(data.valid.labels)))[40;1Hsession = tf.Session()
x = tf.placeholder(tf.float32, shape=[[1m[96mNone[m, img_size,img_size,num_channels], name=[95m'x'[m)

[96m## labels[m
y_true = tf.placeholder(tf.float32, shape=[[1m[96mNone[m, num_classes], name=[95m'y_true'[m)
y_true_cls = tf.argmax(y_true, dimension=[95m1[m)[46;73H1,1[11CTop"train.py" 205L, 6694C[50C   [11C   [1;1H[?12l[?25hP+q436f\P+q6b75\P+q6b64\P+q6b72\P+q6b6c\P+q2332\P+q2334\P+q2569\P+q2a37\P+q6b31\[?25l[46;73H2,1[11CTop[2;1H[?12l[?25h[?25l[46;73H3[3;1H[?12l[?25h[?25l[46;73H4[4;1H[?12l[?25h[?25l[46;73H5[5;1H[?12l[?25h[?25l[46;73H6[6;1H[?12l[?25h[?25l[46;73H7[7;1H[?12l[?25h[?25l[46;73H8,0-1[8;1H[?12l[?25h[?25l[46;73H9,1  [9;1H[?12l[?25h[?25l[46;73H10,1[10;1H[?12l[?25h[?25l[46;74H1[11;1H[?12l[?25h[?25l[46;74H2[12;1H[?12l[?25h[?25l[46;74H3[13;1H[?12l[?25h[?25l[46;74H4,0-1[14;1H[?12l[?25h[?25l[46;74H5[15;1H[?12l[?25h[?25l[46;74H6,1  [16;1H[?12l[?25h[?25l[46;74H7,0-1[17;1H[?12l[?25h[?25l[46;74H8,1  [18;1H[?12l[?25h[?25l[46;74H9[19;1H[?12l[?25h[?25l[46;73H20[20;1H[?12l[?25h[?25l[46;74H1,0-1[21;1H[?12l[?25h[?25l[46;74H2,1  [22;1H[?12l[?25h[?25l[46;74H3[23;1H[?12l[?25h[?25l[46;74H4[24;1H[?12l[?25h[?25l[46;74H5[25;1H[?12l[?25h[?25l[46;74H6[26;1H[?12l[?25h[?25l[46;74H7,0-1[27;1H[?12l[?25h[?25l[46;74H8,1  [28;1H[?12l[?25h[?25l[46;74H9[30;1H[?12l[?25h[?25l[46;73H30,0-1[32;1H[?12l[?25h[?25l[46;74H1[33;1H[?12l[?25h[?25l[46;74H2,1  [34;1H[?12l[?25h[?25l[46;74H3[35;1H[?12l[?25h[?25l[46;74H4[36;1H[?12l[?25h[?25l[46;74H5,0-1[37;1H[?12l[?25h[?25l[46;74H6[38;1H[?12l[?25h[?25l[46;74H7[39;1H[?12l[?25h[?25l[46;74H8,1  [40;1H[?12l[?25h[?25l[46;74H9[41;1H[?12l[?25h[?25l[46;73H40,0-1[42;1H[?12l[?25h[?25l[46;74H1,1  [43;1H[?12l[?25h[?25l[46;74H2[44;1H[?12l[?25h[?25l[46;74H3[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[46;1H[K[46;73H44,0-1[9C0%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[46;73H[K[46;73H45,0-1[9C1%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[46;73H[K[46;73H46,0-1[9C1%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[45;1H[96m##Network graph params[m[46;73H[K[46;73H47,1[11C2%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[45;1Hfilter_size_conv1 = [95m3[m[46;73H[K[46;73H48,1[11C3%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[45;1Hnum_filters_conv1 = [95m32[m[46;73H[K[46;73H49,1[11C3%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[46;73H[K[46;73H50,0-1[9C4%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[45;1Hfilter_size_conv2 = [95m3[m[46;73H[K[46;73H51,1[11C4%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[45;1Hnum_filters_conv2 = [95m32[m[46;73H[K[46;73H52,1[11C5%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[46;73H[K[46;73H53,0-1[9C6%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[45;1Hfilter_size_conv3 = [95m3[m[46;73H[K[46;73H54,1[11C6%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[45;1Hnum_filters_conv3 = [95m64[m[46;73H[K[46;73H55,1[11C7%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[46;73H[K[46;73H56,1[11C8%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[45;1Hfc_layer_size = [95m128[m[46;73H[K[46;73H57,1[11C8%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[46;73H[K[46;73H58,0-1[9C9%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[45;1H[93mdef[m [1m[96mcreate_weights[m(shape):[46;73H[K[46;73H59,1[11C9%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[45;5H[93mreturn[m tf.Variable(tf.truncated_normal(shape, stddev=[95m0.05[m))[46;73H[K[46;73H60,1[10C10%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[46;73H[K[46;73H61,0-1[8C11%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[45;1H[93mdef[m [1m[96mcreate_biases[m(size):[46;73H[K[46;73H62,1[10C11%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[45;5H[93mreturn[m tf.Variable(tf.constant([95m0.05[m, shape=[size]))[46;73H[K[46;73H63,1[10C12%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[46;73H[K[46;73H64,0-1[8C12%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[46;73H[K[46;73H65,0-1[8C13%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[46;73H[K[46;73H66,0-1[8C14%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[45;1H[93mdef[m [1m[96mcreate_convolutional_layer[m([1m[96minput[m,[46;73H[K[46;73H67,1[10C14%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[45;16Hnum_input_channels,[46;73H[K[46;73H68,1[10C15%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[45;16Hconv_filter_size,[46;73H[K[46;73H69,1[10C16%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[45;16Hnum_filters):[46;73H[K[46;73H70,1[10C16%[45;1H[?12l[?25h[?25l[1;45r[1;1H[2M[1;46r[45;5H[96m## We shall define the weights that will be trained using create_weights function.[m[46;73H[K[46;73H71,1[10C17%[44;1H[?12l[?25h[?25l[46;74H2[45;1H[?12l[?25h[?25l[1;45r[1;1H[2M[1;46r[44;5Hweights = create_weights(shape=[conv_filter_size, conv_filter_size, num_input_channelss[45;1H, num_filters])[46;73H[K[46;73H73,1[10C18%[44;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[45;5H[96m## We create biases using the create_biases function. These are also trained.[m[46;73H[K[46;73H74,1[10C18%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[45;5Hbiases = create_biases(num_filters)[46;73H[K[46;73H75,1[10C19%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[46;73H[K[46;73H76,0-1[8C19%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[45;5H[96m## Creating the convolutional layer[m[46;73H[K[46;73H77,1[10C20%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[45;5Hlayer = tf.nn.conv2d([1m[96minput[m=[1m[96minput[m,[46;73H[K[46;73H78,1[10C21%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[45;22H[1m[96mfilter[m=weights,[46;73H[K[46;73H79,1[10C21%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[45;22Hstrides=[[95m1[m, [95m1[m, [95m1[m, [95m1[m],[46;73H[K[46;73H80,1[10C22%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[45;22Hpadding=[95m'SAME'[m)[46;73H[K[46;73H81,1[10C22%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[46;73H[K[46;73H82,0-1[8C23%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[45;5Hlayer += biases[46;73H[K[46;73H83,1[10C24%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[46;73H[K[46;73H84,0-1[8C24%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[45;5H[96m## We shall be using max-pooling.  [m[46;73H[K[46;73H85,1[10C25%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[45;5Hlayer = tf.nn.max_pool(value=layer,[46;73H[K[46;73H86,1[10C26%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[45;29Hksize=[[95m1[m, [95m2[m, [95m2[m, [95m1[m],[46;73H[K[46;73H87,1[10C26%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[45;29Hstrides=[[95m1[m, [95m2[m, [95m2[m, [95m1[m],[46;73H[K[46;73H88,1[10C27%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[45;29Hpadding=[95m'SAME'[m)[46;73H[K[46;73H89,1[10C27%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[45;5H[96m## Output of pooling is fed to Relu which is the activation function for us.[m[46;73H[K[46;73H90,1[10C28%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[45;5Hlayer = tf.nn.relu(layer)[46;73H[K[46;73H91,1[10C29%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[46;73H[K[46;73H92,0-1[8C29%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[45;5H[93mreturn[m layer[46;73H[K[46;73H93,1[10C30%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[46;73H[K[46;73H94,0-1[8C31%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[46;73H[K[46;73H95,1[10C31%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[46;73H[K[46;73H96,0-1[8C32%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[45;1H[93mdef[m [1m[96mcreate_flatten_layer[m(layer):[46;73H[K[46;73H97,1[10C32%[45;1H[?12l[?25h[?25l[1;45r[1;1H[2M[1;46r[44;5H[96m#We know that the shape of the layer will be [batch_size img_size img_size num_channell[45;1Hs] [m[46;73H[K[46;73H98,1[10C33%[44;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[45;5H[96m# But let's get it from the previous layer.[m[46;73H[K[46;73H99,1[10C34%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[45;5Hlayer_shape = layer.get_shape()[46;73H[K[46;73H100,1[9C35%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[46;73H[K[46;73H101,0-1[7C35%[45;1H[?12l[?25h[?25l[1;45r[1;1H[2M[1;46r[44;5H[96m## Number of features will be img_height * img_width* num_channels. But we shall calcuu[45;1Hlate it in place of hard-coding it.[m[46;73H[K[46;73H102,1[9C36%[44;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[45;5Hnum_features = layer_shape[[95m1[m:[95m4[m].num_elements()[46;73H[K[46;73H103,1[9C37%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[46;73H[K[46;73H104,0-1[7C38%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[45;5H[96m## Now, we Flatten the layer so we shall have to reshape to num_features[m[46;73H[K[46;73H105,1[9C38%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[45;5Hlayer = tf.reshape(layer, [-[95m1[m, num_features])[46;73H[K[46;73H106,1[9C39%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[46;73H[K[46;73H107,0-1[7C39%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[45;5H[93mreturn[m layer[46;73H[K[46;73H108,1[9C40%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[46;73H[K[46;73H109,0-1[7C41%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[46;73H[K[46;73H110,0-1[7C41%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[45;1H[93mdef[m [1m[96mcreate_fc_layer[m([1m[96minput[m,[46;73H[K[46;73H111,1[9C42%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[45;14Hnum_inputs,[46;73H[K[46;73H112,1[9C42%[45;1H[?12l[?25h[?25l[1;45r[45;1H
[1;46r[45;14Hnum_outputs,[46;73H[K[46;73H113,1[9C43%[45;1H[?12l[?25h[?25l[46;73H[K[46;1H:[?12l[?25h [?25l[71C113,1[9C43%[45;1H[?12l[?25h[?25l
< >  32,  Hex 20,  Octal 040[46;73H[K[46;73H113,1[9C43%[45;1H[?12l[?25h[?25l[27m[23m[m[H[2J[1;1Haccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))


session.run(tf.global_variables_initializer())


[93mdef[m [1m[96mshow_progress[m(epoch, feed_dict_train, feed_dict_validate, val_loss):
    acc = session.run(accuracy, feed_dict=feed_dict_train)
    val_acc = session.run(accuracy, feed_dict=feed_dict_validate)
    msg = [95m"Training Epoch {0} --- Training Accuracy: {1:>6.1%}, Validation Accuracy: {2:>66[11;1H.1%},  Validation Loss: {3:.3f}"[m
    [1m[96mprint[m(msg.format(epoch + [95m1[m, acc, val_acc, val_loss))

total_iterations = [95m0[m

saver = tf.train.Saver()
[93mdef[m [1m[96mtrain[m(num_iteration):
    [93mglobal[m total_iterations[20;5H[93mfor[m i [93min[m [1m[96mrange[m(total_iterations,[21;20Htotal_iterations + num_iteration):[23;9Hx_batch, y_true_batch, _, cls_batch = data.train.next_batch(batch_size)[24;9Hx_valid_batch, y_valid_batch, _, valid_cls_batch = data.valid.next_batch(batch_sizz[25;1He)[28;9Hfeed_dict_tr = {x: x_batch,[29;28Hy_true: y_true_batch}[30;9Hfeed_dict_val = {x: x_valid_batch,[31;31Hy_true: y_valid_batch}[33;9Hsession.run(optimizer, feed_dict=feed_dict_tr)[35;9H[93mif[m i % [1m[96mint[m(data.train.num_examples/batch_size) == [95m0[m:[36;13Hval_loss = session.run(cost, feed_dict=feed_dict_val)[37;13Hepoch = [1m[96mint[m(i / [1m[96mint[m(data.train.num_examples/batch_size))[39;13Hshow_progress(epoch, feed_dict_tr, feed_dict_val, val_loss)[40;13Hsaver.save(session, [95m"./test_model.ckpt"[m)[43;5Htotal_iterations += num_iteration

train(num_iteration=[95m20000[m)[46;73H205,1[9CBot[45;1H[?12l[?25h[?25l
[1m-- INSERT --[m[46;73H[K[46;73H205,27[8CBot[45;6H[46m([19C)[?12l[?25h[?25l[m[46;78H6[45;26H[?12l[?25h[?25ltrain([19C)[46;78H5[45;25H[?12l[?25h[?25l[46;78H4[45;24H[?12l[?25h[?25l[46;78H3[45;23H[?12l[?25h[?25l[46;78H2[45;22H[?12l[?25h[?25l[95m0[m[3C)[45;26H[K[46;78H1[45;21H[?12l[?25h[?25l[95m40000[m)[46;78H2[45;22H[?12l[?25h[46;1H[K[45;21H[?25l[46;73H205,21[8CBot[45;21H[?12l[?25h[?25l[46;73H[K[46;1H:[?12l[?25hw[?25l[?12l[?25hq[?25l[?12l[?25h[?25l"train.py" 205L, 6694C written
[?1l>[?12l[?25h[?1049l]0;sid@blueberry ~/tensorflow2.7/projects/tutorials/Tensorflow-tutorials/tutorial-2-image-classifier[01;32msid@blueberry[00m [01;34m~/tensorflow2.7/projects/tutorials/Tensorflow-tutorials/tutorial-2-image-classsifier $[00m vim train.py [C[C[C[C[C[C[C[C[C[Cls[Kcd ../../ls[Kvim train.py [C[C[C[C[C[C[C[C[C[C[Kpython r[Ktrain.py 
Going to read training images
Now going to read dogs files (Index: 0)
Now going to read cats files (Index: 1)
Complete reading input data. Will Now print a snippet of it
Number of files in Training-set:		26507
Number of files in Validation-set:	6626
2018-01-24 10:07:38.596818: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-01-24 10:07:42.113631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-01-24 10:07:42.120416: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085
pciBusID: 0000:01:00.0
totalMemory: 5.93GiB freeMemory: 5.57GiB
2018-01-24 10:07:42.125975: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)
WARNING:tensorflow:From train.py:43: calling argmax (from tensorflow.python.ops.math_ops) with dimension is deprecated and will be removed in a future version.
Instructions for updating:
Use the `axis` argument instead
Training Epoch 1 --- Training Accuracy:  46.9%, Validation Accuracy:  43.8%,  Validation Loss: 0.699
Training Epoch 2 --- Training Accuracy:  46.9%, Validation Accuracy:  50.0%,  Validation Loss: 0.691
Training Epoch 3 --- Training Accuracy:  53.1%, Validation Accuracy:  59.4%,  Validation Loss: 0.685
Training Epoch 4 --- Training Accuracy:  59.4%, Validation Accuracy:  56.2%,  Validation Loss: 0.679
Training Epoch 5 --- Training Accuracy:  53.1%, Validation Accuracy:  50.0%,  Validation Loss: 0.675
Training Epoch 6 --- Training Accuracy:  50.0%, Validation Accuracy:  65.6%,  Validation Loss: 0.671
Training Epoch 7 --- Training Accuracy:  53.1%, Validation Accuracy:  56.2%,  Validation Loss: 0.668
Training Epoch 8 --- Training Accuracy:  59.4%, Validation Accuracy:  56.2%,  Validation Loss: 0.666
Training Epoch 9 --- Training Accuracy:  50.0%, Validation Accuracy:  59.4%,  Validation Loss: 0.667
Training Epoch 10 --- Training Accuracy:  53.1%, Validation Accuracy:  59.4%,  Validation Loss: 0.670
Training Epoch 11 --- Training Accuracy:  50.0%, Validation Accuracy:  65.6%,  Validation Loss: 0.678
Training Epoch 12 --- Training Accuracy:  56.2%, Validation Accuracy:  65.6%,  Validation Loss: 0.681
Training Epoch 13 --- Training Accuracy:  56.2%, Validation Accuracy:  65.6%,  Validation Loss: 0.687
Training Epoch 14 --- Training Accuracy:  50.0%, Validation Accuracy:  62.5%,  Validation Loss: 0.701
Training Epoch 15 --- Training Accuracy:  50.0%, Validation Accuracy:  62.5%,  Validation Loss: 0.720
Training Epoch 16 --- Training Accuracy:  53.1%, Validation Accuracy:  65.6%,  Validation Loss: 0.737
Training Epoch 17 --- Training Accuracy:  50.0%, Validation Accuracy:  65.6%,  Validation Loss: 0.718
Training Epoch 18 --- Training Accuracy:  56.2%, Validation Accuracy:  62.5%,  Validation Loss: 0.771
Training Epoch 19 --- Training Accuracy:  46.9%, Validation Accuracy:  59.4%,  Validation Loss: 0.718
Training Epoch 20 --- Training Accuracy:  53.1%, Validation Accuracy:  59.4%,  Validation Loss: 0.758
Training Epoch 21 --- Training Accuracy:  59.4%, Validation Accuracy:  62.5%,  Validation Loss: 0.759
Training Epoch 22 --- Training Accuracy:  56.2%, Validation Accuracy:  59.4%,  Validation Loss: 0.707
Training Epoch 23 --- Training Accuracy:  56.2%, Validation Accuracy:  53.1%,  Validation Loss: 0.812
Training Epoch 24 --- Training Accuracy:  56.2%, Validation Accuracy:  62.5%,  Validation Loss: 0.844
Training Epoch 25 --- Training Accuracy:  62.5%, Validation Accuracy:  56.2%,  Validation Loss: 0.875
Training Epoch 26 --- Training Accuracy:  56.2%, Validation Accuracy:  53.1%,  Validation Loss: 0.762
Training Epoch 27 --- Training Accuracy:  59.4%, Validation Accuracy:  50.0%,  Validation Loss: 0.893
Training Epoch 28 --- Training Accuracy:  68.8%, Validation Accuracy:  53.1%,  Validation Loss: 0.976
Training Epoch 29 --- Training Accuracy:  62.5%, Validation Accuracy:  50.0%,  Validation Loss: 0.939
Training Epoch 30 --- Training Accuracy:  65.6%, Validation Accuracy:  56.2%,  Validation Loss: 0.972
Training Epoch 31 --- Training Accuracy:  75.0%, Validation Accuracy:  46.9%,  Validation Loss: 0.888
Training Epoch 32 --- Training Accuracy:  84.4%, Validation Accuracy:  50.0%,  Validation Loss: 0.958
Training Epoch 33 --- Training Accuracy:  81.2%, Validation Accuracy:  46.9%,  Validation Loss: 0.967
Training Epoch 34 --- Training Accuracy:  87.5%, Validation Accuracy:  50.0%,  Validation Loss: 0.949
Training Epoch 35 --- Training Accuracy:  90.6%, Validation Accuracy:  50.0%,  Validation Loss: 0.919
Training Epoch 36 --- Training Accuracy:  90.6%, Validation Accuracy:  50.0%,  Validation Loss: 0.950
Training Epoch 37 --- Training Accuracy:  93.8%, Validation Accuracy:  46.9%,  Validation Loss: 0.988
Training Epoch 38 --- Training Accuracy:  93.8%, Validation Accuracy:  50.0%,  Validation Loss: 0.963
Training Epoch 39 --- Training Accuracy:  93.8%, Validation Accuracy:  46.9%,  Validation Loss: 1.125
Training Epoch 40 --- Training Accuracy:  93.8%, Validation Accuracy:  43.8%,  Validation Loss: 1.084
Training Epoch 41 --- Training Accuracy:  93.8%, Validation Accuracy:  43.8%,  Validation Loss: 1.043
Training Epoch 42 --- Training Accuracy:  93.8%, Validation Accuracy:  53.1%,  Validation Loss: 1.039
Training Epoch 43 --- Training Accuracy:  93.8%, Validation Accuracy:  46.9%,  Validation Loss: 1.029
Training Epoch 44 --- Training Accuracy:  96.9%, Validation Accuracy:  50.0%,  Validation Loss: 1.002
Training Epoch 45 --- Training Accuracy:  96.9%, Validation Accuracy:  50.0%,  Validation Loss: 0.985
Training Epoch 46 --- Training Accuracy:  93.8%, Validation Accuracy:  56.2%,  Validation Loss: 1.091
Training Epoch 47 --- Training Accuracy:  96.9%, Validation Accuracy:  53.1%,  Validation Loss: 1.181
Training Epoch 48 --- Training Accuracy:  93.8%, Validation Accuracy:  65.6%,  Validation Loss: 1.135
Training Epoch 49 --- Training Accuracy:  93.8%, Validation Accuracy:  59.4%,  Validation Loss: 1.084
]0;sid@blueberry ~/tensorflow2.7/projects/tutorials/Tensorflow-tutorials/tutorial-2-image-classifier[01;32msid@blueberry[00m [01;34m~/tensorflow2.7/projects/tutorials/Tensorflow-tutorials/tutorial-2-image-classsifier $[00m [K[A]0;sid@blueberry ~/tensorflow2.7/projects/tutorials/Tensorflow-tutorials/tutorial-2-image-classifier[01;32msid@blueberry[00m [01;34m~/tensorflow2.7/projects/tutorials/Tensorflow-tutorials/tutorial-2-image-classifier $[00m exit
exit


